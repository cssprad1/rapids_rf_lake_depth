{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "# Import all modules for RF (RAPIDS, DASK, etc)\n",
    "from load_dataset import custom_lakedepth as ld\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestRegressor as cumlDaskRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "\n",
    "c\n",
    "\n",
    "data = ld.LakeDepth(42)\n",
    "covariates_train, covariates_test, labels_train, labels_test = data.split(0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions = n_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(5)\n",
    "results = []\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "# Introduce lists to the hyperparameters we want to\n",
    "\n",
    "N_ESTIMATORS = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "SPLIT_ALGO = 1\n",
    "SPLIT_CRITERION = 2\n",
    "BOOTSTRAP = [True, False]\n",
    "BOOTSTRAP_FEATURES = False\n",
    "ROWS_SAMPLE = 1.0\n",
    "MAX_DEPTH = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "MAX_LEAVES = -1\n",
    "MAX_FEATURES = ['auto', 'sqrt']\n",
    "N_BINS = [int(x) for x in np.linspace(start=5, stop=20, num=10)]\n",
    "MIN_ROWS_PER_NODE = 2\n",
    "MIN_IMPURITY_DECREASE = 0.0\n",
    "ACCURACY_METRIC = 'mean_ae'  # 'mse' #'r2' # 'median_aw' #\n",
    "QUANTILEPT = False\n",
    "SEED = 42\n",
    "VERBOSE = False\n",
    "\n",
    "random_grid = {'n_estimators': N_ESTIMATORS,\n",
    "               'max_depth': MAX_DEPTH,\n",
    "               'bootstrap': BOOTSTRAP,\n",
    "               'max_features': MAX_FEATURES,\n",
    "               'n_bins': N_BINS}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"  - from Random Search: Epoch #\", i)\n",
    "    depth_rf_model = cumlDaskRF(n_estimators=random.choice(N_ESTIMATORS),\n",
    "                          split_algo=SPLIT_ALGO,\n",
    "                          split_criterion=SPLIT_CRITERION,\n",
    "                          bootstrap=random.choice(BOOTSTRAP),\n",
    "                          bootstrap_features=BOOTSTRAP_FEATURES,\n",
    "                          rows_sample=ROWS_SAMPLE,\n",
    "                          max_depth=random.choice(MAX_DEPTH),\n",
    "                          max_leaves=MAX_LEAVES,\n",
    "                          max_features=random.choice(MAX_FEATURES),\n",
    "                          n_bins=random.choice(N_BINS),\n",
    "                          min_rows_per_node=MIN_ROWS_PER_NODE,\n",
    "                          min_impurity_decrease=MIN_IMPURITY_DECREASE,\n",
    "                          accuracy_metric=ACCURACY_METRIC,\n",
    "                          quantile_per_tree=QUANTILEPT,\n",
    "                          seed=SEED,\n",
    "                          verbose=VERBOSE)\n",
    "\n",
    "    scores = []\n",
    "    st = time.time()\n",
    "\n",
    "    for k, (train, test) in enumerate(k_fold.split(covariates_train, labels_train)):\n",
    "       \n",
    "        X_dask_cudf = dask_cudf.from_cudf(covariates_train.iloc[train], npartitions=n_workers)\n",
    "        y_dask_cudf = dask_cudf.from_cudf(labels_train.iloc[train], npartitions=n_workers)\n",
    "        X_dask_cudf, y_dask_cudf = persist_across_workers(dask_client,\n",
    "                                                  [X_dask_cudf,\n",
    "                                                   y_dask_cudf])\n",
    "        depth_rf_model.fit(cv_train, l_train)\n",
    "        wait(depth_rf_model.rfs)\n",
    "        predictions = depth_rf_model.predict(covariates_test)\n",
    "        score = mean_absolute_error(labels_test, pred)\n",
    "        # score = depth_rf_model.score(covariates.iloc[test],\n",
    "        # labels.iloc[test])\n",
    "        scores.append(score)\n",
    "\n",
    "        et = time.time()\n",
    "        print(\"   -time to train (sec): \", et-st)\n",
    "\n",
    "    results.append({'n_estimators': depth_rf_model.n_estimators,\n",
    "                    'bootstrap': depth_rf_model.bootstrap,\n",
    "                    'max_depth': depth_rf_model.max_depth,\n",
    "                    'max_features': depth_rf_model.max_features,\n",
    "                    'n_bins': depth_rf_model.n_bins,\n",
    "                    'performance': np.mean(scores)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myRapids]",
   "language": "python",
   "name": "conda-env-.conda-myRapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
