{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sytem and python modules\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "# Import RAPIDS specific modules\n",
    "\n",
    "import cudf as df\n",
    "import cuml\n",
    "from cuml import train_test_split\n",
    "from cuml.metrics.regression import r2_score as r2d2\n",
    "from cuml.ensemble import RandomForestRegressor as clRF\n",
    "\n",
    "# Import sklearn specific modules\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Import data-visualization modules\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some globals variables and paths\n",
    "FEATURES_PATH = '../data/pts_merged_final.csv'\n",
    "DEPTH = 'Depth_m'\n",
    "DATE = 'Date'\n",
    "FID = 'FID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything into GPU-based DF\n",
    "lakes_depth_df = df.read_csv(FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary values from DF\n",
    "lakes_depth_nd = lakes_depth_df.drop(['FID', 'Date'], axis = 1)\n",
    "lakes_depth_nd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our acutal_predictions i.e. labels and our covariates dataframes\n",
    "labels = lakes_depth_nd['Depth_m']\n",
    "covariates = lakes_depth_nd.drop(['Depth_m'], axis=1)\n",
    "\n",
    "# Check to ensure everything looks good\n",
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we change all our covariate and label data to float32\n",
    "\n",
    "labels = labels.astype(cp.float32)\n",
    "covariates = covariates.astype(cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce lists to the hyperparameters we want to \n",
    "\n",
    "N_ESTIMATORS = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "SPLIT_ALGO = 1\n",
    "SPLIT_CRITERION = 2\n",
    "BOOTSTRAP = [True, False]\n",
    "BOOTSTRAP_FEATURES = False\n",
    "ROWS_SAMPLE = 1.0\n",
    "MAX_DEPTH = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "MAX_LEAVES = -1\n",
    "MAX_FEATURES = ['auto', 'sqrt']\n",
    "N_BINS = [int(x) for x in np.linspace(start = 5, stop = 20, num = 10)]\n",
    "MIN_ROWS_PER_NODE = 2\n",
    "MIN_IMPURITY_DECREASE = 0.0\n",
    "ACCURACY_METRIC = 'mean_ae' # 'mse' #'r2' # 'median_aw' # \n",
    "QUANTILEPT = False\n",
    "SEED = 42\n",
    "VERBOSE = False\n",
    "\n",
    "random_grid = {'n_estimators' : N_ESTIMATORS,\n",
    "              'max_depth' : MAX_DEPTH,\n",
    "              'bootstrap' : BOOTSTRAP,\n",
    "              'max_features': MAX_FEATURES,\n",
    "              'n_bins' : N_BINS}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_(N_SEARCH):\n",
    "    k_fold = KFold(5)\n",
    "    results = []\n",
    "\n",
    "    for i in range(N_SEARCH):\n",
    "        print(\"Random search epoch: \", i)\n",
    "        depth_rf_model = clRF(n_estimators = random.choice(N_ESTIMATORS), \n",
    "                            split_algo = SPLIT_ALGO, \n",
    "                            split_criterion = SPLIT_CRITERION, \n",
    "                            bootstrap = random.choice(BOOTSTRAP),\n",
    "                            bootstrap_features = BOOTSTRAP_FEATURES, \n",
    "                            rows_sample = ROWS_SAMPLE,\n",
    "                            max_depth = random.choice(MAX_DEPTH), \n",
    "                            max_leaves = MAX_LEAVES, \n",
    "                            max_features = random.choice(MAX_FEATURES),\n",
    "                            n_bins = random.choice(N_BINS),\n",
    "                            min_rows_per_node = MIN_ROWS_PER_NODE,\n",
    "                            min_impurity_decrease = MIN_IMPURITY_DECREASE,\n",
    "                            accuracy_metric = ACCURACY_METRIC,\n",
    "                            quantile_per_tree = QUANTILEPT,\n",
    "                            seed = SEED,\n",
    "                            verbose = VERBOSE)\n",
    "\n",
    "        # Split the data to train and test, shuffle to prevent overfitting\n",
    "        st = time.time()\n",
    "        cv_train, cv_test, labels_train, labels_test = train_test_split(covariates, labels,\n",
    "                                                               test_size=TEST_SIZE, \n",
    "                                                               shuffle=True,\n",
    "                                                               random_state=RANDOM_STATE)\n",
    "        et = time.time()\n",
    "        print(\"   -time to split data (sec): \", et-st)\n",
    "        \n",
    "        # Fit the model to new parameters\n",
    "        st = time.time()\n",
    "        depth_rf_model.fit(cv_train, labels_train)\n",
    "        et = time.time()\n",
    "        print(\"   -time to train (sec): \", et-st)\n",
    "        \n",
    "        score = depth_rf_model.score(cv_test, labels_test)\n",
    "        print(\"   -score (mae): \", score)\n",
    "\n",
    "        results.append({'n_estimators':depth_rf_model.n_estimators, 'bootstrap':depth_rf_model.bootstrap,\n",
    "                  'max_depth':depth_rf_model.max_depth, 'max_features':depth_rf_model.max_features,\n",
    "                  'n_bins':depth_rf_model.n_bins, 'performance':score})\n",
    "        \n",
    "        results.sort(key=lambda x : -x['performace'])\n",
    "        \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_kfold(N_SEARCH):\n",
    "\n",
    "    k_fold = KFold(5)\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for i in range(N_SEARCH):\n",
    "        print(\"Random search epoch: \", i)\n",
    "        depth_rf_model = clRF(n_estimators = random.choice(N_ESTIMATORS), \n",
    "                            split_algo = SPLIT_ALGO, \n",
    "                            split_criterion = SPLIT_CRITERION, \n",
    "                            bootstrap = random.choice(BOOTSTRAP),\n",
    "                            bootstrap_features = BOOTSTRAP_FEATURES, \n",
    "                            rows_sample = ROWS_SAMPLE,\n",
    "                            max_depth = random.choice(MAX_DEPTH), \n",
    "                            max_leaves = MAX_LEAVES, \n",
    "                            max_features = random.choice(MAX_FEATURES),\n",
    "                            n_bins = random.choice(N_BINS),\n",
    "                            min_rows_per_node = MIN_ROWS_PER_NODE,\n",
    "                            min_impurity_decrease = MIN_IMPURITY_DECREASE,\n",
    "                            accuracy_metric = ACCURACY_METRIC,\n",
    "                            quantile_per_tree = QUANTILEPT,\n",
    "                            seed = SEED,\n",
    "                            verbose = VERBOSE)\n",
    "\n",
    "        scores = []\n",
    "        st = time.time()\n",
    "        for k, (train, test) in enumerate(k_fold.split(covariates, labels)):\n",
    "            depth_rf_model.fit(covariates.iloc[train], labels.iloc[train])\n",
    "            score = depth_rf_model.score(covariates.iloc[test], labels.iloc[test])\n",
    "            scores.append(score)\n",
    "        et = time.time()\n",
    "        print(\"   -time to train (sec): \", et-st)\n",
    "\n",
    "        results.append({'n_estimators':depth_rf_model.n_estimators, 'bootstrap':depth_rf_model.bootstrap,\n",
    "                  'max_depth':depth_rf_model.max_depth, 'max_features':depth_rf_model.max_features,\n",
    "                  'n_bins':depth_rf_model.n_bins, 'performance':np.mean(scores)})\n",
    "        \n",
    "        results.sort(key=lambda x : -x['performance'])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_search_100 = random_search_kfold(100)\n",
    "print(rs_search_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myRapids]",
   "language": "python",
   "name": "conda-env-.conda-myRapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
