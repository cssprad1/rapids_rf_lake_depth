{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and cuDF working together\n",
    "\n",
    "Converting some of the code to work with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sytem and python modules\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "# Import RAPIDS specific modules\n",
    "\n",
    "import cudf as df\n",
    "import cuml\n",
    "from cuml import train_test_split\n",
    "from cuml.metrics.regression import r2_score as r2d2\n",
    "\n",
    "# Import Dask specific modules\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestRegressor as cumlDaskRF\n",
    "\n",
    "# Import sklearn specific modules\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Import data-visualization modules\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some globals variables and paths\n",
    "FEATURES_PATH = '../data/pts_merged_final.csv'\n",
    "DEPTH = 'Depth_m'\n",
    "DATE = 'Date'\n",
    "FID = 'FID'\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything into GPU-based DF\n",
    "lakes_depth_df = df.read_csv(FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary values from DF\n",
    "lakes_depth_nd = lakes_depth_df.drop(['FID', 'Date'], axis = 1)\n",
    "lakes_depth_nd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data for any anomolies or anything else odd-looking\n",
    "lakes_depth_nd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our acutal_predictions i.e. labels and our covariates dataframes\n",
    "labels = lakes_depth_nd['Depth_m']\n",
    "covariates = lakes_depth_nd.drop(['Depth_m'], axis=1)\n",
    "\n",
    "# Check to ensure everything looks good\n",
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we change all our covariate and label data to float32\n",
    "\n",
    "labels = labels.astype(cp.float32)\n",
    "covariates = covariates.astype(cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train, cv_test, labels_train, labels_test = train_test_split(covariates, labels,\n",
    "                                                               test_size=TEST_SIZE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the right size and shapes on our split data\n",
    "print('Training features shape:', cv_train.shape)\n",
    "print('Testing features shape:', cv_test.shape)\n",
    "print('Training labels shape:', labels_train.shape)\n",
    "print('Testing labels shape:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribute data to worker GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions = n_workers\n",
    "\n",
    "def distribute(covariates, labels):\n",
    "\n",
    "    # Partition with Dask\n",
    "    # In this case, each worker will train on 1/n_partitions fraction of the data\n",
    "    cv_dask = dask_cudf.from_cudf(covariates, npartitions=n_partitions)\n",
    "    labels_dask = dask_cudf.from_cudf(labels, npartitions=n_partitions)\n",
    "\n",
    "    # Persist to cache the data in active memory\n",
    "    cv_dask, labels_dask = \\\n",
    "      dask_utils.persist_across_workers(c, [cv_dask, labels_dask], workers=workers)\n",
    "    \n",
    "    return cv_dask, labels_dask\n",
    "\n",
    "cv_train_dask, labels_train_dask = distribute(cv_train, labels_train)\n",
    "cv_test_dask, labels_test_dask = distribute(cv_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the distributed cuML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some global variables for training phase\n",
    "\n",
    "# Hyper-paramters\n",
    "N_ESTIMATORS = 2000\n",
    "SPLIT_ALGO = 1\n",
    "SPLIT_CRITERION = 2\n",
    "BOOTSTRAP = True\n",
    "BOOTSTRAP_FEATURES = False\n",
    "ROWS_SAMPLE = 1.0\n",
    "MAX_DEPTH = 16\n",
    "MAX_LEAVES = -1\n",
    "MAX_FEATURES = 'auto'\n",
    "N_BINS = 8\n",
    "MIN_ROWS_PER_NODE = 2\n",
    "MIN_IMPURITY_DECREASE = 0.0\n",
    "ACCURACY_METRIC = 'mean_ae' # 'mse' #'r2' # 'median_aw' # \n",
    "QUANTILEPT = False\n",
    "SEED = 42\n",
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_rf_model_0 = cumlDaskRF(n_estimators = N_ESTIMATORS, \n",
    "                        split_algo = SPLIT_ALGO, \n",
    "                        split_criterion = SPLIT_CRITERION, \n",
    "                        bootstrap = BOOTSTRAP,\n",
    "                        bootstrap_features = BOOTSTRAP_FEATURES, \n",
    "                        rows_sample = ROWS_SAMPLE,\n",
    "                        max_depth = MAX_DEPTH, \n",
    "                        max_leaves = MAX_LEAVES, \n",
    "                        max_features = MAX_FEATURES,\n",
    "                        n_bins = N_BINS,\n",
    "                        min_rows_per_node = MIN_ROWS_PER_NODE,\n",
    "                        min_impurity_decrease = MIN_IMPURITY_DECREASE,\n",
    "                        accuracy_metric = ACCURACY_METRIC,\n",
    "                        quantile_per_tree = QUANTILEPT,\n",
    "                        seed = SEED,\n",
    "                        verbose = VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "depth_rf_model_0.fit(cv_train_dask, labels_train_dask)\n",
    "wait(depth_rf_model_0.rfs) # Allow asynchronous training tasks to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml_y_pred = depth_rf_model_0.predict(cv_test_dask).compute().to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some prediction\n",
    "from sklearn.metrics import mean_absolute_error as m_a_e, r2_score as r2d2\n",
    "\n",
    "mae_score = m_a_e(labels_test.to_array(), cuml_y_pred)\n",
    "r2_score = r2d2(labels_test.to_array(), cuml_y_pred)\n",
    "print(\"Scores --\")\n",
    "print(\"MAE: \", mae_score)\n",
    "print(\"r2: \", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rapids-0.16]",
   "language": "python",
   "name": "conda-env-.conda-rapids-0.16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
